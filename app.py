import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import streamlit as st
import re
import json
from collections import Counter
from typing import List, Dict, Optional
from datetime import datetime
import math
import os
import shutil
import gc
import time
from contextlib import contextmanager
import logging
import warnings
from pathlib import Path
from dotenv import load_dotenv

# Suppress warnings
warnings.filterwarnings('ignore')

# Check and remove models directory
if os.path.exists('models'):
    shutil.rmtree('models')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set page config at the very top
st.set_page_config(
    page_title="Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Initialize session state
if 'generation_count' not in st.session_state:
    st.session_state.generation_count = 0
if 'last_generation_time' not in st.session_state:
    st.session_state.last_generation_time = 0

# Constants
MAX_GENERATIONS_PER_MINUTE = 10
GENERATION_COOLDOWN = 6  # seconds
MODEL_TIMEOUT = 30  # seconds
MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 16
LEARNING_RATE = 0.0001
NUM_EPOCHS = 5

# Utility Functions
@contextmanager
def timer(name):
    start_time = time.time()
    yield
    end_time = time.time()
    logger.info(f"{name} took {end_time - start_time:.2f} seconds")

def cleanup_gpu_memory():
    """Clean up GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def rate_limit_check() -> bool:
    """Check if we should rate limit the generation"""
    current_time = time.time()
    if current_time - st.session_state.last_generation_time < GENERATION_COOLDOWN:
        return False
    if st.session_state.generation_count >= MAX_GENERATIONS_PER_MINUTE:
        return False
    return True

def safe_load_file(file_path: str) -> bool:
    """Safely load a file with error handling"""
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        return True
    except Exception as e:
        logger.error(f"Error loading file: {str(e)}")
        return False
    
class ArabicTextProcessor:
    def __init__(self):
        self.arabic_chars = set('Ø§Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ')
        self.tashkeel = set('Ù‹ÙŒÙÙÙÙÙ‘Ù’')
        self.numbers = set('0123456789Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©')
        self.punctuation = set('ØŒØ›ØŸ.')
        
    def normalize_arabic(self, text: str) -> str:
        """Normalize Arabic text"""
        try:
            # Remove tashkeel
            text = ''.join([c for c in text if c not in self.tashkeel])
            
            # Normalize characters
            text = re.sub('[Ø£Ø¥Ø¢Ø§]', 'Ø§', text)
            text = text.replace('Ø©', 'Ù‡')
            text = text.replace('Ù‰', 'ÙŠ')
            
            return text
        except Exception as e:
            logger.error(f"Error in normalize_arabic: {str(e)}")
            return text

    def clean_text(self, text: str) -> str:
        """Clean and prepare text"""
        try:
            # Remove extra whitespace
            text = ' '.join(text.split())
            
            # Keep only allowed characters
            allowed_chars = self.arabic_chars | self.numbers | self.punctuation | {' '}
            text = ''.join([c for c in text if c in allowed_chars])
            
            return text
        except Exception as e:
            logger.error(f"Error in clean_text: {str(e)}")
            return text

class ArabicTokenizer:
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.processor = ArabicTextProcessor()
        self.word_freq = Counter()
        
        # Special tokens
        self.PAD_token = '[PAD]'
        self.UNK_token = '[UNK]'
        self.SOS_token = '[SOS]'
        self.EOS_token = '[EOS]'
        
        # Initialize vocabularies with common Arabic real estate terms
        self.word2idx = {
            self.PAD_token: 0,
            self.UNK_token: 1,
            self.SOS_token: 2,
            self.EOS_token: 3,
            # Common real estate terms
            'Ø¹Ù‚Ø§Ø±': 4,
            'ÙÙŠÙ„Ø§': 5,
            'Ø´Ù‚Ø©': 6,
            'Ù…Ø¬Ù…Ø¹': 7,
            'Ø³ÙƒÙ†ÙŠ': 8,
            'ØªØ¬Ø§Ø±ÙŠ': 9,
            'Ø§Ù„Ø±ÙŠØ§Ø¶': 10,
            'Ø¬Ø¯Ø©': 11,
            'Ø§Ù„Ø¯Ù…Ø§Ù…': 12,
            'Ù…ÙˆÙ‚Ø¹': 13,
            'Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ': 14,
            'Ù…Ù…ÙŠØ²': 15,
            'ÙØ§Ø®Ø±': 16,
            'Ø§Ø³ØªØ«Ù…Ø§Ø±': 17,
            'Ø¹Ø§Ø¦Ø¯': 18,
            'Ø±ÙŠØ§Ù„': 19,
            'Ù…ØªØ±': 20,
            'ØºØ±Ù': 21,
            'Ø­Ù…Ø§Ù…Ø§Øª': 22,
            'Ù…Ø·Ø¨Ø®': 23,
            'ØµØ§Ù„Ø©': 24,
            'Ù…Ø¬Ù„Ø³': 25,
            'Ø­Ø¯ÙŠÙ‚Ø©': 26,
            'Ù…Ø³Ø¨Ø­': 27,
            'Ù…ÙˆÙ‚Ù': 28,
            'Ø³ÙŠØ§Ø±Ø§Øª': 29,
            'Ù…ØµØ¹Ø¯': 30,
            'ØªÙƒÙŠÙŠÙ': 31,
            'Ù…Ø±ÙƒØ²ÙŠ': 32,
            'ØªØ´Ø·ÙŠØ¨': 33,
            'ÙØ§Ø®Ø±': 34,
            'Ù‚Ø±ÙŠØ¨': 35,
            'Ù…Ù†': 36,
            'ÙÙŠ': 37,
            'Ø¥Ù„Ù‰': 38,
            'Ù…Ø¹': 39,
            'Ø¹Ù„Ù‰': 40,
            'Ø§Ù„Ø®Ø¯Ù…Ø§Øª': 41,
            'Ø§Ù„Ù…Ø¯Ø§Ø±Ø³': 42,
            'Ø§Ù„Ù…Ø³ØªØ´ÙÙŠØ§Øª': 43,
            'Ø§Ù„Ø£Ø³ÙˆØ§Ù‚': 44,
            'Ø§Ù„Ù…Ø³Ø§Ø¬Ø¯': 45,
            'Ø§Ù„Ø­Ø¯Ø§Ø¦Ù‚': 46,
            'Ø§Ù„Ø¹Ø§Ù…Ø©': 47,
            'Ø´Ù…Ø§Ù„': 48,
            'Ø¬Ù†ÙˆØ¨': 49,
            'Ø´Ø±Ù‚': 50,
            'ØºØ±Ø¨': 51,
            'ÙˆØ³Ø·': 52,
            'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': 53,
            'Ø§Ù„Ø­ÙŠ': 54,
            'Ø§Ù„Ø³Ø¹Ø±': 55,
            'Ù…Ù„ÙŠÙˆÙ†': 56,
            'Ø£Ù„Ù': 57,
            'Ù…Ø³Ø§Ø­Ø©': 58,
            'ÙˆØ§Ø¬Ù‡Ø©': 59,
            'Ø´Ø§Ø±Ø¹': 60,
            'Ø±Ø¦ÙŠØ³ÙŠ': 61,
            'ÙØ±Ø¹ÙŠ': 62,
            'Ø¬Ø¯ÙŠØ¯': 63,
            'ØªØ­Øª': 64,
            'Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡': 65,
            'Ø¬Ø§Ù‡Ø²': 66,
            'Ù„Ù„Ø³ÙƒÙ†': 67,
            'Ø¶Ù…Ø§Ù†': 68,
            'ØµÙŠØ§Ù†Ø©': 69,
            'Ø³Ù†Ø¯': 70,
            'Ù…Ù„ÙƒÙŠØ©': 71,
            'ØªÙ…ÙˆÙŠÙ„': 72,
            'Ø¨Ù†ÙƒÙŠ': 73,
            'Ø¯ÙØ¹Ø§Øª': 74,
            'Ù…ÙŠØ³Ø±Ø©': 75,
            # Add more relevant terms...
        }
        
        # Create reverse mapping
        self.idx2word = {v: k for k, v in self.word2idx.items()}

    def build_vocabulary(self, texts: List[str]):
        """Build vocabulary from texts"""
        try:
            # Reset word frequency counter
            self.word_freq = Counter()
            
            for text in texts:
                cleaned_text = self.processor.clean_text(
                    self.processor.normalize_arabic(text)
                )
                self.word_freq.update(cleaned_text.split())
            
            # Select top words
            top_words = [word for word, _ in 
                        self.word_freq.most_common(self.vocab_size - 4)]
            
            # Reset vocabulary
            self.word2idx = {
                self.PAD_token: 0,
                self.UNK_token: 1,
                self.SOS_token: 2,
                self.EOS_token: 3,
            }
            
            # Add words to vocabulary
            for word in top_words:
                if word not in self.word2idx:
                    idx = len(self.word2idx)
                    self.word2idx[word] = idx
                    
            # Update reverse mapping
            self.idx2word = {v: k for k, v in self.word2idx.items()}
                
        except Exception as e:
            logger.error(f"Error in build_vocabulary: {str(e)}")
            raise

    def encode_plus(self, text: str, max_length: int = MAX_SEQUENCE_LENGTH, 
                   padding: str = 'max_length', return_tensors: str = 'pt') -> Dict:
        """Encode text to model inputs"""
        try:
            # Process text
            text = self.processor.normalize_arabic(text)
            text = self.processor.clean_text(text)
            words = text.split()
            
            # Convert to indices
            indices = [self.word2idx.get(word, self.word2idx[self.UNK_token]) 
                      for word in words]
            indices = [self.word2idx[self.SOS_token]] + indices + [self.word2idx[self.EOS_token]]
            
            # Handle length
            if len(indices) > max_length:
                indices = indices[:max_length]
            
            # Padding
            attention_mask = [1] * len(indices)
            if padding == 'max_length':
                pad_length = max_length - len(indices)
                indices.extend([self.word2idx[self.PAD_token]] * pad_length)
                attention_mask.extend([0] * pad_length)
            
            # Convert to tensors if requested
            if return_tensors == 'pt':
                return {
                    'input_ids': torch.tensor([indices]),
                    'attention_mask': torch.tensor([attention_mask])
                }
            
            return {
                'input_ids': indices,
                'attention_mask': attention_mask
            }
            
        except Exception as e:
            logger.error(f"Error in encode_plus: {str(e)}")
            raise

    def decode(self, indices: List[int]) -> str:
        """Decode indices to text"""
        try:
            words = []
            for idx in indices:
                word = self.idx2word.get(idx, self.UNK_token)
                if word in [self.PAD_token, self.SOS_token, self.EOS_token]:
                    continue
                words.append(word)
            return ' '.join(words)
        except Exception as e:
            logger.error(f"Error in decode: {str(e)}")
            return ""

    def save(self, path: str):
        """Save tokenizer to file"""
        try:
            save_dict = {
                'vocab_size': self.vocab_size,
                'word2idx': self.word2idx,
                'idx2word': {str(k): v for k, v in self.idx2word.items()},
                'word_freq': dict(self.word_freq)
            }
            
            # Ensure directory exists
            os.makedirs(os.path.dirname(path), exist_ok=True)
            
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(save_dict, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"Error saving tokenizer: {str(e)}")
            raise

    @classmethod
    def load(cls, path: str) -> 'ArabicTokenizer':
        """Load tokenizer from file"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                save_dict = json.load(f)
            
            tokenizer = cls(vocab_size=save_dict['vocab_size'])
            tokenizer.word2idx = save_dict['word2idx']
            tokenizer.idx2word = {int(k): v for k, v in save_dict['idx2word'].items()}
            tokenizer.word_freq = Counter(save_dict['word_freq'])
            
            return tokenizer
            
        except Exception as e:
            logger.error(f"Error loading tokenizer: {str(e)}")
            raise

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        # Change from register_buffer to Parameter to ensure state_dict saving
        self.pe = nn.Parameter(pe, requires_grad=False)

    def forward(self, x):
        return x + self.pe[:x.size(0)]
    
class LightweightArabicModel(nn.Module):
    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 4, 
                 num_layers: int = 3, dropout: float = 0.1):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=1024,
            dropout=dropout,
            batch_first=True
        )
        
        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
        self.d_model = d_model
        self.vocab_size = vocab_size

    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        try:
            src = self.embedding(src) * math.sqrt(self.d_model)
            src = self.pos_encoder(src)
            src = self.dropout(src)
            
            output = self.transformer(src, src_mask, src_key_padding_mask)
            output = self.fc_out(output)
            
            return output
        except Exception as e:
            logger.error(f"Error in model forward pass: {str(e)}")
            raise

class OptimizedRealEstateDataset(Dataset):
    def __init__(self, tokenizer: ArabicTokenizer, max_length: int = MAX_SEQUENCE_LENGTH):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        self.templates = {
            "locations": {
                "Ø§Ù„Ø±ÙŠØ§Ø¶": ["Ø´Ù…Ø§Ù„", "Ø¬Ù†ÙˆØ¨", "Ø´Ø±Ù‚", "ØºØ±Ø¨", "ÙˆØ³Ø·"],
                "Ø¬Ø¯Ø©": ["Ø§Ù„Ø­Ù…Ø±Ø§Ø¡", "Ø§Ù„Ø¨Ù„Ø¯", "Ø§Ù„Ø´Ø§Ø·Ø¦", "Ø§Ù„Ù†Ø²Ù‡Ø©"],
                "Ø§Ù„Ø¯Ù…Ø§Ù…": ["Ø§Ù„Ø´Ø§Ø·Ø¦", "Ø§Ù„Ù…Ø±ÙƒØ²", "Ø§Ù„Ø¶Ø§Ø­ÙŠØ©"],
            },
            "features": [
                "Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ",
                "ØªØ´Ø·ÙŠØ¨Ø§Øª ÙØ§Ø®Ø±Ø©",
                "Ù…Ø³Ø§Ø­Ø§Øª ÙˆØ§Ø³Ø¹Ø©",
                "Ø¥Ø·Ù„Ø§Ù„Ø© Ù…Ù…ÙŠØ²Ø©",
                "Ø®Ø¯Ù…Ø§Øª Ù…ØªÙƒØ§Ù…Ù„Ø©",
                "ØªØµÙ…ÙŠÙ… Ø¹ØµØ±ÙŠ",
                "Ù…ÙˆØ§Ù‚Ù Ø³ÙŠØ§Ø±Ø§Øª",
                "Ø£Ù…Ù† ÙˆØ­Ø±Ø§Ø³Ø©",
            ],
            "property_types": [
                "Ø´Ù‚Ø© Ø³ÙƒÙ†ÙŠØ©",
                "ÙÙŠÙ„Ø§",
                "Ù…Ø¬Ù…Ø¹ Ø³ÙƒÙ†ÙŠ",
            ]
        }

    def generate_synthetic_data(self, num_samples: int = 500):
        """Generate synthetic training data"""
        try:
            for _ in range(num_samples):
                property_type = np.random.choice(self.templates["property_types"])
                location = np.random.choice(list(self.templates["locations"].keys()))
                area = np.random.choice(self.templates["locations"][location])
                
                # Select random features
                features = np.random.choice(
                    self.templates["features"],
                    size=np.random.randint(3, 6),
                    replace=False
                ).tolist()
                
                # Generate price
                price = f"{np.random.randint(300, 2000)} Ø£Ù„Ù Ø±ÙŠØ§Ù„"
                
                # Create description
                description = f"""
                Ø¹Ù‚Ø§Ø± Ù…Ù…ÙŠØ² ÙÙŠ {location} - {area}
                Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: {property_type}
                Ø§Ù„Ø³Ø¹Ø±: {price}
                
                Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
                - {features[0]}
                - {features[1]}
                - {features[2]}
                
                ÙŠØªÙ…ÙŠØ² Ø§Ù„Ø¹Ù‚Ø§Ø± Ø¨Ù…ÙˆÙ‚Ø¹Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙÙŠ {area} {location}ØŒ
                ÙˆÙŠÙˆÙØ± {', Ùˆ'.join(features)}.
                
                Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø³ÙƒÙ† ÙˆØ§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±.
                """
                
                self.data.append({'text': description.strip()})
                
        except Exception as e:
            logger.error(f"Error generating synthetic data: {str(e)}")
            raise

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        try:
            encoding = self.tokenizer.encode_plus(
                self.data[idx]['text'],
                max_length=self.max_length,
                padding='max_length',
                return_tensors='pt'
            )
            
            return {
                'input_ids': encoding['input_ids'].squeeze(0),
                'attention_mask': encoding['attention_mask'].squeeze(0)
            }
        except Exception as e:
            logger.error(f"Error getting dataset item: {str(e)}")
            raise

class RealEstateTextGenerator:
    def __init__(self, model: LightweightArabicModel, tokenizer: ArabicTokenizer, 
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()

    def generate_text(self, prompt: str, max_length: int = 150) -> str:
        """Enhanced text generation method"""
        try:
            # Prepare template-based responses
            templates = {
                "case_study": """
                Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø© ØªÙØµÙŠÙ„ÙŠØ© Ù„Ù„Ø¹Ù‚Ø§Ø±:
                Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: {property_type}
                Ø§Ù„Ù…ÙˆÙ‚Ø¹: {location}
                
                ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙˆÙ‚:
                - Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙÙŠ {location}
                - Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                - Ù…Ù†Ø·Ù‚Ø© Ù…ØªÙ†Ø§Ù…ÙŠØ© Ø¹Ù…Ø±Ø§Ù†ÙŠØ§Ù‹
                - Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©
                
                ÙØ±Øµ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±:
                - Ø¹Ø§Ø¦Ø¯ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ Ù…ØªÙˆÙ‚Ø¹ 15-20%
                - Ù†Ù…Ùˆ Ø³Ù†ÙˆÙŠ ÙÙŠ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±
                - Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ£Ø¬ÙŠØ± Ø¨Ø¹Ø§Ø¦Ø¯ Ù…Ø¬Ø²ÙŠ
                - ÙØ±Øµ ØªØ·ÙˆÙŠØ± Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                """,
                
                "property_description": """
                {property_type} Ù…Ù…ÙŠØ²Ø© ÙÙŠ {location}
                
                Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
                - Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ
                - ØªØ´Ø·ÙŠØ¨Ø§Øª ÙØ§Ø®Ø±Ø©
                - Ù…Ø³Ø§Ø­Ø§Øª ÙˆØ§Ø³Ø¹Ø©
                - Ø®Ø¯Ù…Ø§Øª Ù…ØªÙƒØ§Ù…Ù„Ø©
                
                Ø§Ù„Ù…Ø±Ø§ÙÙ‚:
                - Ù…ÙˆØ§Ù‚Ù Ø³ÙŠØ§Ø±Ø§Øª
                - Ø­Ø¯Ø§Ø¦Ù‚
                - Ù†Ø¸Ø§Ù… Ø£Ù…Ù†ÙŠ Ù…ØªÙƒØ§Ù…Ù„
                - Ù…ØµØ§Ø¹Ø¯ Ø­Ø¯ÙŠØ«Ø©
                """,
                
                "investment_analysis": """
                ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ:
                
                - Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±: 2-3 Ù…Ù„ÙŠÙˆÙ† Ø±ÙŠØ§Ù„
                - Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø³Ù†ÙˆÙŠ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: 8%
                - ÙØªØ±Ø© Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„: 7-8 Ø³Ù†ÙˆØ§Øª
                - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ø³Ù†ÙˆÙŠ: 5%
                """
            }
            
            # Select appropriate template based on prompt content
            if "Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø©" in prompt:
                template = templates["case_study"]
            elif "ØªØ­Ù„ÙŠÙ„" in prompt:
                template = templates["investment_analysis"]
            else:
                template = templates["property_description"]
            
            # Extract property type and location from prompt
            property_type = re.search(r'Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: (\w+)', prompt)
            location = re.search(r'Ø§Ù„Ù…ÙˆÙ‚Ø¹: (\w+)', prompt)
            
            # Format template
            response = template.format(
                property_type=property_type.group(1) if property_type else "Ø¹Ù‚Ø§Ø±",
                location=location.group(1) if location else "Ù…ÙˆÙ‚Ø¹ Ù…Ù…ÙŠØ²"
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error in text generation: {str(e)}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ"

    def generate_case_study(self, property_type: str, location: str) -> str:
        prompt = f"""
        Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø© ØªÙØµÙŠÙ„ÙŠØ©
        Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: {property_type}
        Ø§Ù„Ù…ÙˆÙ‚Ø¹: {location}
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© ÙˆÙØ±Øµ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±
        """
        return self.generate_text(prompt, max_length=500)

    def generate_pros_cons(self, property_type: str, location: str) -> dict:
        pros = {
            "1": "Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ",
            "2": "Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ÙˆØµÙˆÙ„",
            "3": "Ù‚Ø±Ø¨ Ø§Ù„Ø®Ø¯Ù…Ø§Øª",
            "4": "Ø¹Ø§Ø¦Ø¯ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ Ù…Ø±ØªÙØ¹"
        }
        
        cons = {
            "1": "Ø§Ù„Ù…Ù†Ø§ÙØ³Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©",
            "2": "ØªÙƒØ§Ù„ÙŠÙ Ø§Ù„ØµÙŠØ§Ù†Ø©",
            "3": "ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø³ÙˆÙ‚",
            "4": "Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©"
        }
        
        return {"pros": pros, "cons": cons}

    def generate_target_audience(self) -> str:
        return "Ø§Ù„Ù…Ø³ØªØ«Ù…Ø±ÙˆÙ† Ù…Ù† Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¹Ù…Ø±ÙŠØ© 35-50 Ø³Ù†Ø©ØŒ Ø°ÙˆÙŠ Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø±ØªÙØ¹"

    def generate_roi_analysis(self) -> str:
        return "Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ 15-20% Ø³Ù†ÙˆÙŠØ§Ù‹"

    def generate_kpis(self) -> dict:
        return {
            "1": "Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø´ØºØ§Ù„",
            "2": "Ù…ØªÙˆØ³Ø· Ø³Ø¹Ø± Ø§Ù„Ø¥ÙŠØ¬Ø§Ø±",
            "3": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø³Ù†ÙˆÙŠ",
            "4": "ÙØªØ±Ø© Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„"
        }

    def generate_hashtags(self, platform: str) -> List[str]:
        hashtags = [
            "#Ø¹Ù‚Ø§Ø±Ø§Øª_Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
            "#Ø§Ø³ØªØ«Ù…Ø§Ø±_Ø¹Ù‚Ø§Ø±ÙŠ",
            "#Ø¹Ù‚Ø§Ø±Ø§Øª",
            "#ÙØ±Øµ_Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ©",
            "#Ø¹Ù‚Ø§Ø±Ø§Øª_ÙØ§Ø®Ø±Ø©",
            "#Ø¹Ù‚Ø§Ø±Ø§Øª_Ù„Ù„Ø¨ÙŠØ¹",
            "#Ø§Ø³ØªØ«Ù…Ø§Ø±_Ø¢Ù…Ù†",
            "#Ø³ÙˆÙ‚_Ø§Ù„Ø¹Ù‚Ø§Ø±",
            "#ØªØ·ÙˆÙŠØ±_Ø¹Ù‚Ø§Ø±ÙŠ",
            "#Ø§ÙØ¶Ù„_Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"
        ]
        return hashtags

    def generate_marketing_budget(self) -> dict:
        return {
            "total_budget": "50000 Ø±ÙŠØ§Ù„",
            "facebook_ads": "15000 Ø±ÙŠØ§Ù„",
            "instagram_ads": "15000 Ø±ÙŠØ§Ù„",
            "twitter_ads": "10000 Ø±ÙŠØ§Ù„",
            "linkedin_ads": "10000 Ø±ÙŠØ§Ù„"
        }

    def generate_social_media_plan(self, property_type: str, location: str) -> dict:
        platforms = ["facebook", "instagram", "twitter", "linkedin"]
        plan = {}
        
        base_posts = [
            "ÙØ±ØµØ© Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ© Ù…Ù…ÙŠØ²Ø© ÙÙŠ {location}",
            "Ø¹Ù‚Ø§Ø± ÙØ§Ø®Ø± Ù„Ù„Ø¨ÙŠØ¹ ÙÙŠ Ø£Ø±Ù‚Ù‰ Ø£Ø­ÙŠØ§Ø¡ {location}",
            "Ø§Ø³ØªØ«Ù…Ø± ÙÙŠ {property_type} Ø¨Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ",
            "Ø¹Ø§Ø¦Ø¯ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ Ù…Ø¶Ù…ÙˆÙ† ÙÙŠ {location}",
            "ÙØ±ØµØ© Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙÙŠ Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªÙ†Ø§Ù…ÙŠ",
            "Ù…ÙˆÙ‚Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙŠØ¶Ù…Ù† Ù†Ù…Ùˆ Ø§Ø³ØªØ«Ù…Ø§Ø±Ùƒ",
            "Ø§Ø³ØªØ«Ù…Ø± ÙÙŠ Ù…Ø³ØªÙ‚Ø¨Ù„Ùƒ Ù…Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª",
            "Ø¹Ù‚Ø§Ø± ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ÙØ®Ø§Ù…Ø© ÙˆØ§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¢Ù…Ù†",
            "Ù…ÙˆÙ‚Ø¹ Ù…Ù…ÙŠØ² ÙˆØªØ´Ø·ÙŠØ¨Ø§Øª Ø±Ø§Ù‚ÙŠØ©",
            "Ø§Ø³ØªØ«Ù…Ø§Ø± ÙŠØ¶Ù…Ù† Ù…Ø³ØªÙ‚Ø¨Ù„Ùƒ Ø§Ù„Ù…Ø§Ù„ÙŠ"
        ]
        
        for platform in platforms:
            posts = []
            for i, base_post in enumerate(base_posts, 1):
                post = base_post.format(
                    location=location,
                    property_type=property_type
                )
                posts.append({f"Post{i}": post})
            plan[platform] = posts
        
        return plan

    def generate_marketing_plan(self, property_type: str, location: str) -> dict:
        try:
            response = {
                "Case_Study": self.generate_case_study(property_type, location),
                "Pros": self.generate_pros_cons(property_type, location)["pros"],
                "Cons": self.generate_pros_cons(property_type, location)["cons"],
                "Target_Audience": self.generate_target_audience(),
                "ROI_Analysis": self.generate_roi_analysis(),
                "KPIs": self.generate_kpis(),
                "Social_Media_Plan": self.generate_social_media_plan(property_type, location),
                "Hashtags": {
                    platform: self.generate_hashtags(platform)
                    for platform in ["Facebook", "Instagram", "Twitter", "LinkedIn"]
                },
                "Marketing_Budget": self.generate_marketing_budget()
            }
            return response
        except Exception as e:
            logger.error(f"Error in marketing plan generation: {str(e)}")
            raise

@st.cache_resource
def load_or_train_models():
    try:
        model_path = 'models/model_checkpoint.pt'
        tokenizer_path = 'models/tokenizer.json'
        
        # Create models directory if it doesn't exist
        os.makedirs('models', exist_ok=True)
        
        if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):
            with st.spinner("Training new model... This may take a few minutes."):
                model, tokenizer = train_model()
                st.success("Model training completed!")
        else:
            with st.spinner("Loading existing model..."):
                # Load tokenizer first
                tokenizer = ArabicTokenizer.load(tokenizer_path)
                
                # Load checkpoint
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                checkpoint = torch.load(model_path, map_location=device)
                
                # Create model with tokenizer's vocabulary size
                model = LightweightArabicModel(
                    vocab_size=len(tokenizer.word2idx),
                    d_model=256,
                    nhead=4,
                    num_layers=3
                )
                
                # Load state dict
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()
                model = model.to(device)
                
                st.success("Model loaded successfully!")
        
        return tokenizer, model
        
    except Exception as e:
        logger.error(f"Error in model loading/training: {str(e)}")
        st.error(f"Error: {str(e)}")
        st.stop()

def train_model(save_dir='models'):
    try:
        # Create models directory if it doesn't exist
        os.makedirs(save_dir, exist_ok=True)
        
        # Initialize tokenizer and dataset
        tokenizer = ArabicTokenizer(vocab_size=10000)
        dataset = OptimizedRealEstateDataset(tokenizer)
        dataset.generate_synthetic_data(num_samples=500)
        
        # Build vocabulary
        texts = [item['text'] for item in dataset.data]
        tokenizer.build_vocabulary(texts)
        
        # Save tokenizer
        tokenizer_path = os.path.join(save_dir, 'tokenizer.json')
        tokenizer.save(tokenizer_path)
        
        # Initialize model
        model = LightweightArabicModel(
            vocab_size=len(tokenizer.word2idx),
            d_model=256,
            nhead=4,
            num_layers=3
        )
        
        # Training setup
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        criterion = nn.CrossEntropyLoss(
            ignore_index=tokenizer.word2idx[tokenizer.PAD_token]
        )
        
        # Create data loader
        dataloader = DataLoader(
            dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            num_workers=0 if os.name == 'nt' else 2
        )
        
        # Training progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Training loop
        for epoch in range(NUM_EPOCHS):
            model.train()
            total_loss = 0
            
            for batch_idx, batch in enumerate(dataloader):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                
                targets = input_ids[:, 1:].contiguous()
                inputs = input_ids[:, :-1].contiguous()
                mask = attention_mask[:, :-1].contiguous()
                
                optimizer.zero_grad()
                outputs = model(inputs, src_key_padding_mask=~mask.bool())
                loss = criterion(
                    outputs.view(-1, model.vocab_size),
                    targets.view(-1)
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
                
                # Update progress
                progress = (epoch * len(dataloader) + batch_idx + 1) / (NUM_EPOCHS * len(dataloader))
                progress_bar.progress(progress)
                status_text.text(f"Epoch {epoch+1}/{NUM_EPOCHS}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}")
                
                # Clean up GPU memory
                if device.type == 'cuda':
                    cleanup_gpu_memory()
            
            avg_loss = total_loss / len(dataloader)
            logger.info(f"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}")
        
        # Save final model
        model_path = os.path.join(save_dir, 'model_checkpoint.pt')
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': NUM_EPOCHS,
            'loss': avg_loss
        }, model_path)
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"Error in model training: {str(e)}")
        raise
@st.cache_data(ttl=3600)
def cached_generate_description(_generator, property_type, location):
    try:
        if not property_type or not location:
            raise ValueError("Property type and location are required")
            
        prompt = f"""Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: {property_type}
        Ø§Ù„Ù…ÙˆÙ‚Ø¹: {location}
        Ø§Ù„ÙˆØµÙ:"""
        
        return _generator.generate(prompt, max_length=150)
    except Exception as e:
        st.error(f"Error generating description: {str(e)}")
        return None
# Define the cached function outside the class
class OptimizedRealEstateAIApp:
    def __init__(self):
        try:
            self.tokenizer, self.model = load_or_train_models()
            self.generator = RealEstateTextGenerator(
                self.model, 
                self.tokenizer
            )
        except Exception as e:
            st.error(f"Error initializing app: {str(e)}")
            st.stop()

    def generate_property_description(self, property_type, location):
        try:
            marketing_plan = self.generator.generate_marketing_plan(
                property_type, 
                location
            )
            
            # Convert to JSON string for proper formatting
            return json.dumps(marketing_plan, ensure_ascii=False, indent=2)
            
        except Exception as e:
            st.error(f"Error generating marketing plan: {str(e)}")
            return None

    def run(self):
        try:
            # Set Arabic text direction
            st.markdown("""
                <style>
                .stApp { direction: rtl; }
                .css-18e3th9 { direction: rtl; }
                div[data-testid="stMarkdownContainer"] { direction: rtl; }
                </style>
            """, unsafe_allow_html=True)
            
            # App title
            st.title("ğŸ¢ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
            
            # Input fields
            col1, col2 = st.columns(2)
            with col1:
                location = st.selectbox(
                    "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©",
                    ["Ø§Ù„Ø±ÙŠØ§Ø¶", "Ø¬Ø¯Ø©", "Ø§Ù„Ø¯Ù…Ø§Ù…"],
                    index=None,
                    placeholder="Ø§Ø®ØªØ± Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©"
                )
            with col2:
                property_type = st.selectbox(
                    "Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±",
                    ["Ø´Ù‚Ø© Ø³ÙƒÙ†ÙŠØ©", "ÙÙŠÙ„Ø§", "Ù…Ø¬Ù…Ø¹ Ø³ÙƒÙ†ÙŠ"],
                    index=None,
                    placeholder="Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±"
                )
            
            # Generate button
            if st.button("ØªÙˆÙ„ÙŠØ¯ Ø®Ø·Ø© ØªØ³ÙˆÙŠÙ‚ÙŠØ©"):
                if not location or not property_type:
                    st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±")
                    return
                    
                with st.spinner('Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ø·Ø© Ø§Ù„ØªØ³ÙˆÙŠÙ‚ÙŠØ©...'):
                    marketing_plan = self.generate_property_description(
                        property_type, 
                        location
                    )
                    if marketing_plan:
                        st.markdown("### Ø§Ù„Ø®Ø·Ø© Ø§Ù„ØªØ³ÙˆÙŠÙ‚ÙŠØ©")
                        st.json(marketing_plan)
                        
                        # Add download button
                        st.download_button(
                            "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ø§Ù„ØªØ³ÙˆÙŠÙ‚ÙŠØ©",
                            marketing_plan,
                            f"marketing_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                    
        except Exception as e:
            logger.error(f"Error in app execution: {str(e)}")
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")

if __name__ == "__main__":
    try:
        app = OptimizedRealEstateAIApp()
        app.run()
        
    except Exception as e:
        st.error(f"Application error: {str(e)}")
        logger.error(f"Application error: {str(e)}")
